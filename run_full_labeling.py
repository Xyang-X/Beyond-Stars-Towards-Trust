# run_full_labeling.py - å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡ç­¾
import sys
import os
import json
import pandas as pd
import time
from tqdm import tqdm

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def load_and_process_data(input_path, batch_size=1000):
    """åˆ†æ‰¹åŠ è½½å’Œå¤„ç†æ•°æ®"""
    print(f"ğŸ“ å¼€å§‹å¤„ç†æ–‡ä»¶: {input_path}")
    print(f"ğŸ“Š æ‰¹å¤„ç†å¤§å°: {batch_size}")
    
    # ç»Ÿè®¡æ€»è¡Œæ•°
    print("ğŸ” ç»Ÿè®¡æ–‡ä»¶æ€»è¡Œæ•°...")
    total_lines = 0
    with open(input_path, 'r', encoding='utf-8') as f:
        for _ in f:
            total_lines += 1
    
    print(f"ğŸ“ˆ æ–‡ä»¶æ€»è¡Œæ•°: {total_lines:,}")
    
    # åˆ†æ‰¹å¤„ç†
    all_results = []
    processed_lines = 0
    
    with open(input_path, 'r', encoding='utf-8') as f:
        batch = []
        
        for line_num, line in enumerate(tqdm(f, total=total_lines, desc="å¤„ç†è¿›åº¦")):
            line = line.strip()
            if not line:
                continue
                
            try:
                data = json.loads(line)
                batch.append(data)
                processed_lines += 1
                
                # å½“æ‰¹æ¬¡æ»¡äº†æˆ–åˆ°è¾¾æ–‡ä»¶æœ«å°¾æ—¶å¤„ç†
                if len(batch) >= batch_size or line_num == total_lines - 1:
                    batch_results = process_batch(batch, processed_lines - len(batch) + 1)
                    all_results.extend(batch_results)
                    
                    # æ˜¾ç¤ºè¿›åº¦
                    progress = (processed_lines / total_lines) * 100
                    print(f"ğŸ“Š è¿›åº¦: {processed_lines:,}/{total_lines:,} ({progress:.1f}%) - å·²å¤„ç† {len(all_results):,} æ¡è¯„è®º")
                    
                    # æ¸…ç©ºæ‰¹æ¬¡
                    batch = []
                    
            except json.JSONDecodeError as e:
                print(f"âš ï¸ ç¬¬{line_num+1}è¡ŒJSONè§£æå¤±è´¥: {e}")
                continue
            except Exception as e:
                print(f"âŒ ç¬¬{line_num+1}è¡Œå¤„ç†å¤±è´¥: {e}")
                continue
    
    return all_results

def process_batch(batch, start_index):
    """å¤„ç†ä¸€æ‰¹æ•°æ®"""
    try:
        from lf_rules import (
            lf_promo_has_link, lf_too_short, lf_template_low_entities,
            lf_entity_sparse, lf_offtopic, lf_format_noise, lf_trust_signal,
            lf_rating_sentiment_conflict, lf_suspicious_patterns,
            lf_brand_mentioning, lf_time_sensitive_content, rough_entity_count
        )
        from lf_aggregate import aggregate_lfs
        from config import TAU_HIGH, TAU_LOW
    except ImportError as e:
        print(f"âŒ å¯¼å…¥æ ‡ç­¾å‡½æ•°å¤±è´¥: {e}")
        return []
    
    results = []
    
    for i, row in enumerate(batch):
        try:
            # è·å–æ–‡æœ¬ä¿¡æ¯
            text = row.get('text', row.get('original_text', ''))
            if not text:
                text = row.get('processed_text', '')
            
            # è®¡ç®—æ–‡æœ¬ç‰¹å¾
            len_tok = len(text.split()) if text else 0
            len_char = len(text) if text else 0
            ent_count = rough_entity_count(text) if 'rough_entity_count' in globals() else 0
            
            # è¿è¡Œæ‰€æœ‰æ ‡ç­¾å‡½æ•°
            lf_outputs = {}
            
            # 1. ä¿ƒé”€æ£€æµ‹
            has_url = False  # ç®€åŒ–å¤„ç†
            has_phone = False  # ç®€åŒ–å¤„ç†
            lf_outputs['promo'] = lf_promo_has_link(text, has_url, has_phone)
            
            # 2. é•¿åº¦æ£€æµ‹
            lf_outputs['too_short'] = lf_too_short(len_tok, len_char)
            
            # 3. æ¨¡æ¿æ£€æµ‹
            lf_outputs['template'] = lf_template_low_entities(text, ent_count)
            
            # 4. å®ä½“ç¨€ç–æ£€æµ‹
            lf_outputs['entity_sparse'] = lf_entity_sparse(len_char, ent_count)
            
            # 5. ç¦»é¢˜æ£€æµ‹
            lf_outputs['offtopic'] = lf_offtopic(row.get('category'), text)
            
            # 6. æ ¼å¼å™ªéŸ³æ£€æµ‹
            lf_outputs['format_noise'] = lf_format_noise(text)
            
            # 7. å¯ä¿¡ä¿¡å·æ£€æµ‹
            has_promo_hit = lf_outputs['promo'][0] == 1
            lf_outputs['trust_signal'] = lf_trust_signal(ent_count, has_promo_hit)
            
            # 8. è¯„åˆ†æƒ…æ„Ÿå†²çªæ£€æµ‹
            rating = row.get('rating', 3)
            sent_pos = 0.5  # ç®€åŒ–å¤„ç†
            sent_neg = 0.3  # ç®€åŒ–å¤„ç†
            lf_outputs['sent_conflict'] = lf_rating_sentiment_conflict(rating, sent_pos, sent_neg)
            
            # 9. å¯ç–‘æ¨¡å¼æ£€æµ‹
            lf_outputs['suspicious_patterns'] = lf_suspicious_patterns(text)
            
            # 10. å“ç‰ŒæåŠæ£€æµ‹
            lf_outputs['brand_mentioning'] = lf_brand_mentioning(text)
            
            # 11. æ—¶é—´æ•æ„Ÿå†…å®¹æ£€æµ‹
            lf_outputs['time_sensitive_content'] = lf_time_sensitive_content(text)
            
            # èšåˆæ ‡ç­¾å‡½æ•°è¾“å‡º
            p_untrust, score, hits = aggregate_lfs(lf_outputs)
            
            # æ ¹æ®é˜ˆå€¼ç¡®å®šæœ€ç»ˆæ ‡ç­¾
            if p_untrust >= TAU_HIGH:
                final_label = 1  # ä¸å¯ä¿¡
                label_str = "untrustworthy"
            elif p_untrust <= TAU_LOW:
                final_label = 0  # å¯ä¿¡
                label_str = "trustworthy"
            else:
                final_label = -1  # å¿½ç•¥
                label_str = "ignore"
            
            # è®°å½•ç»“æœ
            result = {
                'comment_id': start_index + i,
                'user_id': row.get('user_id', ''),
                'name': row.get('name', ''),
                'rating': row.get('rating', ''),
                'time': row.get('time', ''),
                'category': row.get('category', ''),
                'robot_review': row.get('robot_review', False),
                'text': text[:200] + "..." if len(text) > 200 else text,
                'processed_text': row.get('processed_text', ''),
                'entity_count': ent_count,
                'len_char': len_char,
                'len_tok': len_tok,
                'p_untrust': p_untrust,
                'score': score,
                'final_label': final_label,
                'label_str': label_str,
                'lf_outputs': lf_outputs
            }
            
            results.append(result)
            
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬{start_index + i}æ¡è¯„è®ºå¤±è´¥: {e}")
            # æ·»åŠ é”™è¯¯è®°å½•
            text_content = row.get('text', row.get('original_text', ''))
            if isinstance(text_content, str) and len(text_content) > 200:
                text_display = text_content[:200] + "..."
            else:
                text_display = str(text_content)
                
            result = {
                'comment_id': start_index + i,
                'user_id': row.get('user_id', ''),
                'name': row.get('name', ''),
                'rating': row.get('rating', ''),
                'time': row.get('time', ''),
                'category': row.get('category', ''),
                'robot_review': row.get('robot_review', False),
                'text': text_display,
                'processed_text': row.get('processed_text', ''),
                'entity_count': 'ERROR',
                'len_char': 'ERROR',
                'len_tok': 'ERROR',
                'p_untrust': 'ERROR',
                'score': 'ERROR',
                'final_label': 'ERROR',
                'label_str': 'ERROR',
                'lf_outputs': {}
            }
            results.append(result)
    
    return results

def save_results(results, output_path):
    """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
    print(f"\nğŸ’¾ ä¿å­˜ç»“æœåˆ°: {output_path}")
    
    # è½¬æ¢ä¸ºDataFrame
    df = pd.DataFrame(results)
    
    # ä¿å­˜ä¸ºCSVæ ¼å¼ï¼ˆä¾¿äºæŸ¥çœ‹ï¼‰
    csv_path = output_path.replace('.parquet', '.csv')
    df.to_csv(csv_path, index=False, encoding='utf-8')
    print(f"âœ… å·²ä¿å­˜ {len(df)} æ¡è¯„è®ºåˆ° {csv_path}")
    
    return df

def generate_summary_report(df):
    """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
    print(f"\nğŸ“Š æ ‡ç­¾ç»“æœæ‘˜è¦æŠ¥å‘Š")
    print("=" * 60)
    
    # åŸºæœ¬ç»Ÿè®¡
    total = len(df)
    untrustworthy = len(df[df['final_label'] == 1])
    trustworthy = len(df[df['final_label'] == 0])
    ignore = len(df[df['final_label'] == -1])
    errors = len(df[df['final_label'] == 'ERROR'])
    
    print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"   æ€»è¯„è®ºæ•°: {total:,}")
    print(f"   å¯ä¿¡è¯„è®º: {trustworthy:,} ({trustworthy/total*100:.1f}%)")
    print(f"   ä¸å¯ä¿¡è¯„è®º: {untrustworthy:,} ({untrustworthy/total*100:.1f}%)")
    print(f"   å¿½ç•¥è¯„è®º: {ignore:,} ({ignore/total*100:.1f}%)")
    print(f"   å¤„ç†å¤±è´¥: {errors:,} ({errors/total*100:.1f}%)")
    
    # æŒ‰è¯„åˆ†ç»Ÿè®¡
    print(f"\nâ­ æŒ‰è¯„åˆ†ç»Ÿè®¡:")
    rating_stats = df[df['final_label'] != 'ERROR'].groupby('rating').agg({
        'final_label': ['count', lambda x: (x == 1).sum(), lambda x: (x == 0).sum()]
    }).round(2)
    
    for rating in sorted(rating_stats.index):
        total_count = rating_stats.loc[rating, ('final_label', 'count')]
        untrust_count = rating_stats.loc[rating, ('final_label', '<lambda_0>')]
        trust_count = rating_stats.loc[rating, ('final_label', '<lambda_1>')]
        
        if total_count > 0:
            untrust_rate = untrust_count / total_count * 100
            trust_rate = trust_count / total_count * 100
            print(f"   è¯„åˆ† {rating}: {untrust_count}/{total_count} ä¸å¯ä¿¡ ({untrust_rate:.1f}%), {trust_count}/{total_count} å¯ä¿¡ ({trust_rate:.1f}%)")
    
    # æŒ‰ä¸šåŠ¡ç±»å‹ç»Ÿè®¡
    print(f"\nğŸª æŒ‰ä¸šåŠ¡ç±»å‹ç»Ÿè®¡:")
    # å¤„ç†categoryå­—æ®µï¼ˆå¯èƒ½æ˜¯åˆ—è¡¨ï¼‰
    df['category_str'] = df['category'].apply(lambda x: str(x[0]) if isinstance(x, list) and x else str(x))
    
    category_stats = df[df['final_label'] != 'ERROR'].groupby('category_str').agg({
        'final_label': ['count', lambda x: (x == 1).sum(), lambda x: (x == 0).sum()]
    }).round(2)
    
    # æ˜¾ç¤ºå‰10ä¸ªæœ€å¸¸è§çš„ä¸šåŠ¡ç±»å‹
    top_categories = category_stats.sort_values(('final_label', 'count'), ascending=False).head(10)
    
    for category in top_categories.index:
        total_count = top_categories.loc[category, ('final_label', 'count')]
        untrust_count = top_categories.loc[category, ('final_label', '<lambda_0>')]
        trust_count = top_categories.loc[category, ('final_label', '<lambda_1>')]
        
        if total_count > 0:
            untrust_rate = untrust_count / total_count * 100
            trust_rate = trust_count / total_count * 100
            print(f"   {category}: {untrust_count}/{total_count} ä¸å¯ä¿¡ ({untrust_rate:.1f}%), {trust_count}/{total_count} å¯ä¿¡ ({trust_rate:.1f}%)")
    
    # æœºå™¨äººè¯„è®ºåˆ†æ
    print(f"\nğŸ¤– æœºå™¨äººè¯„è®ºåˆ†æ:")
    robot_stats = df[df['robot_review'] == True]
    if len(robot_stats) > 0:
        robot_total = len(robot_stats)
        robot_untrust = len(robot_stats[robot_stats['final_label'] == 1])
        robot_trust = len(robot_stats[robot_stats['final_label'] == 0])
        
        print(f"   æœºå™¨äººè¯„è®ºæ€»æ•°: {robot_total:,}")
        print(f"   æœºå™¨äººä¸å¯ä¿¡è¯„è®º: {robot_untrust:,} ({robot_untrust/robot_total*100:.1f}%)")
        print(f"   æœºå™¨äººå¯ä¿¡è¯„è®º: {robot_trust:,} ({robot_trust/robot_total*100:.1f}%)")
    else:
        print("   æœªå‘ç°æœºå™¨äººè¯„è®º")
    
    print(f"\nğŸ¯ æ ‡ç­¾è´¨é‡è¯„ä¼°:")
    print(f"   å¯ä¿¡è¯„è®ºæ¯”ä¾‹: {trustworthy/total*100:.1f}% (ç›®æ ‡: 30-50%)")
    print(f"   ä¸å¯ä¿¡è¯„è®ºæ¯”ä¾‹: {untrustworthy/total*100:.1f}% (ç›®æ ‡: 20-40%)")
    print(f"   å¿½ç•¥è¯„è®ºæ¯”ä¾‹: {ignore/total*100:.1f}% (ç›®æ ‡: æœ€å°åŒ–)")
    
    if trustworthy/total*100 >= 30 and trustworthy/total*100 <= 50:
        print("   âœ… å¯ä¿¡è¯„è®ºæ¯”ä¾‹åœ¨ç›®æ ‡èŒƒå›´å†…")
    else:
        print("   âš ï¸ å¯ä¿¡è¯„è®ºæ¯”ä¾‹è¶…å‡ºç›®æ ‡èŒƒå›´")
    
    if untrustworthy/total*100 >= 20 and untrustworthy/total*100 <= 40:
        print("   âœ… ä¸å¯ä¿¡è¯„è®ºæ¯”ä¾‹åœ¨ç›®æ ‡èŒƒå›´å†…")
    else:
        print("   âš ï¸ ä¸å¯ä¿¡è¯„è®ºæ¯”ä¾‹è¶…å‡ºç›®æ ‡èŒƒå›´")
    
    if ignore/total*100 < 10:
        print("   âœ… å¿½ç•¥è¯„è®ºæ¯”ä¾‹è¾ƒä½")
    else:
        print("   âš ï¸ å¿½ç•¥è¯„è®ºæ¯”ä¾‹è¾ƒé«˜")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œæ ‡ç­¾")
    print("=" * 80)
    
    # æ–‡ä»¶è·¯å¾„
    input_path = r"D:\MyPersonalFiles\NTU\TechJam2025\review-Alaska_10.filtered.redacted.strict.dedup.preprocessed.targeted.json"
    output_path = "full_dataset_labeled.csv"
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not os.path.exists(input_path):
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    
    try:
        # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
        results = load_and_process_data(input_path, batch_size=1000)
        
        # 2. ä¿å­˜ç»“æœ
        df = save_results(results, output_path)
        
        # 3. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        generate_summary_report(df)
        
        # 4. è®¡ç®—æ€»è€—æ—¶
        total_time = time.time() - start_time
        print(f"\nâ±ï¸ æ€»è€—æ—¶: {total_time:.2f} ç§’")
        print(f"ğŸš€ å¤„ç†é€Ÿåº¦: {len(results)/total_time:.0f} æ¡/ç§’")
        
        print(f"\nğŸ‰ æ ‡ç­¾å®Œæˆ!")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶: {output_path}")
        print(f"ğŸ“Š æ€»å¤„ç†è¯„è®ºæ•°: {len(results):,}")
        
    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
